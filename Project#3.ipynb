{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**\n",
    "Return to the writeup you created for project #2. Given what you've just learned about Natural Language Processing, tweak / refine your writeup to incorporate those techniques into your\" fake news detection\" program.<br>\n",
    "Now since we have CountVectorizer and TfidfVectorizer we donot need to construct a vocubulary to compute the features. Now once we have obtained our text data we need to clean the data using word tokenizer removing stop words and punctuations using nltk stop words and punctions from the strings. Once we obtained this data we can use the Stemming / Lematization technique to make  different derivative form of words to a single word.<br>\n",
    "Finall, once we have obtained the cleaned , stemmed data set we can use either Countvectorizer or TfidfVectorizer based on the performance of our classification model's output.If it is giving better for CountVectorizer then we use it otherwise we use TfidfVectorizer.<br>\n",
    "Finally , we can use different model based on performance like Logistic Regression, Random Forest , Gradient Boosting Classifier etc for our prediction.<br>\n",
    "So we take a combination of feature extraction models( i.e. CountVectorizer,TfidfVectorizer) and prediction models (i.e. Logistic Regression, Random Forest , Gradient Boosting Classifier) and check for best performance and choose that combination with best performance over the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"password\",\n",
    "  database=\"news_reports\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Table to dump values from news API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = mydb.cursor()\n",
    "mycursor.execute(\"CREATE TABLE articles ( title text(255))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.1**\n",
    "A standalone Python application (that you create), that streams in news headlines/articles from one or more of the APIs you wrote about, and dumps the results into a relational database of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "query=\"INSERT INTO articles (title) VALUES (%s)\"\n",
    "def get_stream(url):\n",
    "#     global decoded_line\n",
    "    r = requests.get(url, stream=True)\n",
    "    for line in r.iter_lines(chunk_size=10):\n",
    "        if line:\n",
    "            decoded_line = line.decode('utf-8')\n",
    "            dat=json.loads(decoded_line)[\"articles\"]\n",
    "        for lines in dat:\n",
    "            mycursor = mydb.cursor()\n",
    "            val=lines[\"description\"]\n",
    "            if (val,) not in records:\n",
    "                mycursor.execute(query,(val,))\n",
    "                mydb.commit()\n",
    "url='https://newsapi.org/v2/everything?q=google&apiKey=6ad7a293f5534b97951d0377356ae7dd'\n",
    "get_stream(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycursor = mydb.cursor(buffered=True)\n",
    "mycursor.execute(\"SELECT * FROM articles\")\n",
    "records = mycursor.fetchall()\n",
    "type(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2.**\n",
    "A set of commands you can use to dump the database contents into a CSV, containing only the columns that matter to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "my_dict = {'articles': records}\n",
    "dump_db_to_df=pd.DataFrame(my_dict)\n",
    "dump_db_to_df.to_csv('dump_db.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3.**\n",
    "The notebook formulas that can analyze the spam news dataset(s) and learn from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('news_data.csv',usecols=['title','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords_set=set(stopwords)\n",
    "punctuation_set=set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processor(doc):\n",
    "    stops=stopwords\n",
    "    stemmer=SnowballStemmer('english')\n",
    "    doc = word_tokenize(doc.lower())\n",
    "    tokens = [tok for tok in doc if tok not in punctuation_set]\n",
    "    tokens = [tok for tok in tokens if tok]\n",
    "    if stops:\n",
    "        tokens = [tok for tok in tokens if (tok not in stops)]\n",
    "    if stemmer:\n",
    "        tokens = [stemmer.stem(tok) for tok in tokens]\n",
    "    sentence=' '.join([word for word in tokens])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clr_title']=df.title.apply(data_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "count_vect= TfidfVectorizer()\n",
    "X= count_vect.fit_transform(df['clr_title'])\n",
    "y=df.label\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.955456570155902"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg= LogisticRegression(C=5)\n",
    "\n",
    "lg.fit(X_train,y_train)\n",
    "y_pred=lg.predict(X_test)\n",
    "lg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2019,   85],\n",
       "       [ 115, 2271]], dtype=int64)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4.**\n",
    "The notebook formulas that can import the CSV contents from step 2, and analyze the new headlines/articles, then give them a truthfulness rating. For example: \"Likely true\", \"likely false\", etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>Fake/True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Scoreboard at close of play of 3rd t20i betwee...</td>\n",
       "      <td>Likely True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>YouTube livestreaming has been a big promise o...</td>\n",
       "      <td>Likely True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Google announced this morning it’s adding supp...</td>\n",
       "      <td>Likely True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Several new features are showing up in the Goo...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Finally, Google has updated its useful Authent...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Last month, Google announced End-to-End encryp...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Google search’s new online shopping feature ma...</td>\n",
       "      <td>Likely True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Google is giving Pixel 3 devices and later mod...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Google Podcasts just became more alluring if y...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Google has expanded the number of augmented re...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Google Photos will start turning your old imag...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Remember Bixby, Samsung’s voice-activated assi...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Google has switched on Stadia in eight more Eu...</td>\n",
       "      <td>Likely True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Google is making it a lot easier to edit Micro...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Google is once again delaying its return to th...</td>\n",
       "      <td>Likely True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Google is rolling out Chrome OS 87, which incl...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>It’s only been a few months since the US Depar...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Chrome extensions don’t normally create much d...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Texas has kicked off the latest in a slew of l...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Google is adding new games to Assistant-enable...</td>\n",
       "      <td>Likely False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             articles     Fake/True\n",
       "40  Scoreboard at close of play of 3rd t20i betwee...   Likely True\n",
       "41  YouTube livestreaming has been a big promise o...   Likely True\n",
       "42  Google announced this morning it’s adding supp...   Likely True\n",
       "43  Several new features are showing up in the Goo...  Likely False\n",
       "44  Finally, Google has updated its useful Authent...  Likely False\n",
       "45  Last month, Google announced End-to-End encryp...  Likely False\n",
       "46  Google search’s new online shopping feature ma...   Likely True\n",
       "47  Google is giving Pixel 3 devices and later mod...  Likely False\n",
       "48  Google Podcasts just became more alluring if y...  Likely False\n",
       "49  Google has expanded the number of augmented re...  Likely False\n",
       "50  Google Photos will start turning your old imag...  Likely False\n",
       "51  Remember Bixby, Samsung’s voice-activated assi...  Likely False\n",
       "52  Google has switched on Stadia in eight more Eu...   Likely True\n",
       "53  Google is making it a lot easier to edit Micro...  Likely False\n",
       "54  Google is once again delaying its return to th...   Likely True\n",
       "55  Google is rolling out Chrome OS 87, which incl...  Likely False\n",
       "56  It’s only been a few months since the US Depar...  Likely False\n",
       "57  Chrome extensions don’t normally create much d...  Likely False\n",
       "58  Texas has kicked off the latest in a slew of l...  Likely False\n",
       "59  Google is adding new games to Assistant-enable...  Likely False"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_df=pd.read_csv('dump_db.csv')\n",
    "db_df['articles']=db_df.articles.apply(lambda x:x[2:-1])\n",
    "db_df['clr_articles']=db_df['articles'].apply(data_processor)\n",
    "X_db=count_vect.transform(db_df['clr_articles'])\n",
    "db_predict=lg.predict(X_db)\n",
    "db_df['label']=db_predict\n",
    "db_df['Fake/True']=db_df['label'].apply(lambda x:'Likely False' if x==1 else 'Likely True')\n",
    "db_df[['articles','Fake/True']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
